{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb44f6d",
   "metadata": {},
   "source": [
    "## Text Generation using Different Decoding Strategies with Amazon SageMaker JumpStart SDK and Falcon 40B Instruct Language Model\n",
    "\n",
    "---\n",
    "This Amazon SageMaker Studio Notebook demonstrates how to use the SageMaker Python SDK with very little efforts to firstly deploy Falcon-40B-Instruct Large Language Model and then generate text using different decoding methods. \n",
    "\n",
    "This notebook has the following prerequisites:\n",
    "- Select an AWS region where [Amazon SageMaker JumpStart](https://aws.amazon.com/sagemaker/jumpstart) is available. \n",
    "- [Setup Amazon SageMaker Domain](https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html).\n",
    "- [Available service queta for \"ml.g5.12xlarge for endpoint usage\"](https://docs.aws.amazon.com/general/latest/gr/sagemaker.html).\n",
    "- Less than $10 per hour to spend on Amazon SageMaker JumpStart model deployment and Amazon SageMaker Studio notebook usage.  \n",
    "\n",
    "This notebook is based on the following references:\n",
    "- [Amazon SageMaker JumpStart SDK](https://sagemaker.readthedocs.io/en/v2.82.0/overview.html#use-prebuilt-models-with-sagemaker-jumpstart), providing pretrained models for a wide range of problem types to help you get started with machine learning.\n",
    "- [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct), an open source model with 40B parameters decoder-only model built by TII.\n",
    "- Public articles ([Link 1](https://huggingface.co/blog/how-to-generate), [Link 2](https://huggingface.co/docs/transformers/generation_strategies), [Link 3](https://huggingface.co/blog/sagemaker-huggingface-llm#4-run-inference-and-chat-with-our-model)) published on Hugging Face, an open source community and data science platform for machine learning models and datasets. \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b05b931-992e-4526-978d-f03196874a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip --quiet --disable-pip-version-check --root-user-action=ignore\n",
    "!pip install --upgrade sagemaker --quiet --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08508ffa-90f4-4c8c-9c69-b8002bb37f12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model id = huggingface-llm-falcon-40b-instruct-bf16\n",
      "Model name = hf-llm-falcon-40b-instruct-bf16-2023-07-29-10-54-30-085\n",
      "Model version = *\n",
      "Instance type = ml.g5.12xlarge\n",
      "Instance number of GPUs = 4\n",
      "Model maximum input length = 1024\n",
      "Model maximum total tokens = 2048\n",
      "Server endpoint timeout = 300 seconds\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "# Define SageMaker JumpStart Model using model id, instance type, and endpoint timeout\n",
    "my_model = JumpStartModel(model_id=\"huggingface-llm-falcon-40b-instruct-bf16\",\n",
    "                          instance_type=\"ml.g5.12xlarge\",\n",
    "                          env={'ENDPOINT_SERVER_TIMEOUT':'300'})\n",
    "\n",
    "# Take a look at the JumpStart Model parameters printed by this cell\n",
    "print(\"Model id =\", my_model.model_id)\n",
    "print(\"Model name =\", my_model.name)\n",
    "print(\"Model version =\", my_model.model_version)\n",
    "print(\"Instance type =\", my_model.instance_type)\n",
    "print(\"Instance number of GPUs =\", my_model.env[\"SM_NUM_GPUS\"])\n",
    "print(\"Model maximum input length =\", my_model.env[\"MAX_INPUT_LENGTH\"])\n",
    "print(\"Model maximum total tokens =\", my_model.env[\"MAX_TOTAL_TOKENS\"])\n",
    "print(\"Server endpoint timeout =\", my_model.env[\"ENDPOINT_SERVER_TIMEOUT\"], \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a2a8e5-789f-4041-9927-221257126653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------!CPU times: user 153 ms, sys: 25.5 ms, total: 179 ms\n",
      "Wall time: 13min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Host the model on the instance and deploy an inference endpoint\n",
    "# Because the model size is >80GB, expecy deploy() to take 15 min!\n",
    "predictor = my_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b13e2-9d0b-4158-8841-a6c07a7d3b45",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "**Decoding Strategies**\n",
    "\n",
    "Large Language Models are designed to \"guess\" the next token having read all the previous ones based on a predefined sampling method. There are several methods (decoding strategies) we can configure for picking this output token, such as Greedy Search, Beam Search, and Contrastive Search.\n",
    "\n",
    "We start by defining the promote which will enable us to test different decoding strategies for the same input text. We will also fix the values of some hyperparameters across all decoding strategies to make easy for us to compare the generated text:\n",
    "- *Temperature* is used to control the randomness of predictions by scaling the logits before applying softmax. The softmax layer in the transformer architecture turns the logits into probabilities (between 0 and 1). A low temperature (below 1) sharpens the probabilities of the predicted words resulting in more conservative and predictable text. A high temperature (above 1) makes the model generates more creative and diverse text resulting in unusual or unexpected words.\n",
    "- *stop* provides a list of input tokens to the model to stop the generation. The generation will stop when one of the tokens is generated.\n",
    "- *max_new_tokens* defines the maximum number of tokens to be generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77efa7b9-4e50-400b-b7c7-8a8ea07d82a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Did the chicken cross the road while being afraid of cars?\"\n",
    "stop_keywords = [\"<|endoftext|>\", \"</s>\"]\n",
    "max_new_tokens = 120\n",
    "temperature = 1.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22952a-4992-41ab-b4d2-2cfb15559652",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Strategy 1: Greedy Search**\n",
    "\n",
    "Greedy search is a deterministic method that simply selects the word with the highest probability as its next word. To configure greedy decoding, we set the *do_sample* hyperparameter to false and make sure the *num_beams* hyperparameter is set to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1a55aa5-f2ad-4db8-9718-b76f969cffbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did the chicken cross the road while being afraid of cars?\n",
      "Greedy Search Response: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      "\n",
      "I'm sorry, I do not have the ability to determine the actual reasons for why a chicken crosses a road.\n"
     ]
    }
   ],
   "source": [
    "greedy_search_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": False,\n",
    "        \"num_beams\": 1,\n",
    "    }\n",
    "}\n",
    "greedy_search_response = predictor.predict(greedy_search_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Greedy Search Response:\", \">\"*40, \"\\033[95m\")\n",
    "print(greedy_search_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e153983-9964-4e8f-b148-5a2e968d61cc",
   "metadata": {},
   "source": [
    "**Strategy 2: Beam Search**\n",
    "\n",
    "Beam search is another deterministic method that reduces the risk of missing hidden high probability word sequences by keeping a fixed number (beam) of active candidates at each time step and eventually choosing the hypothesis that has the overall highest probability. To configure beam decoding, we set the *do_sample* hyperparameter to false and set the *num_beams* hyperparameter to a value above 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "008e8836-7a49-407f-bf04-8dcf59de8dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did the chicken cross the road while being afraid of cars?\n",
      "Beam Search Response: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      "\n",
      "I'm sorry, as an AI language model, I cannot say for sure whether the chicken crossed the road while being afraid of cars. It depends on its individual circumstances and the context in which it was living.\n"
     ]
    }
   ],
   "source": [
    "beam_search_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": False,\n",
    "        \"num_beams\": 4,\n",
    "    }\n",
    "}\n",
    "beam_search_response = predictor.predict(beam_search_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Beam Search Response:\", \">\"*40, \"\\033[95m\")\n",
    "print(beam_search_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf74caa-c563-4b1e-9585-a28237dc539f",
   "metadata": {},
   "source": [
    "**Strategy 3: Multinomial Sampling**\n",
    "\n",
    "Solely maximizing the output probability in deterministic methods can lead to dullness and repetitions. Conversely, stochastic methods try to solve the problem by introducing randomness to the sampling process. \n",
    "\n",
    "Multinomial sampling randomly selects the next token based on the probability distribution over the entire vocabulary given by the model. Every token with a non-zero probability has a chance of being selected, thus reducing the risk of repetition. To configure multinomial sampling, we set the *do_sample* hyperparameter to true and set the *num_beams* hyperparameter to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b253a50c-a024-4608-9502-00591764eb5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did the chicken cross the road while being afraid of cars?\n",
      "Multinomial Sampling Response: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      "\n",
      "I'm sorry, but as an AI language model, I have zero knowledge of whether chickens are afraid of cars or not. My input is strictly based on the given facts and language models.\n"
     ]
    }
   ],
   "source": [
    "multinomial_sampling_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"num_beams\": 1\n",
    "    }\n",
    "}\n",
    "multinomial_sampling_response = predictor.predict(multinomial_sampling_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Multinomial Sampling Response:\", \">\"*40, \"\\033[95m\")\n",
    "print(multinomial_sampling_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ad903-7381-466a-b0e1-33cd61532786",
   "metadata": {},
   "source": [
    "**Strategy 4: Top-k Sampling**\n",
    "\n",
    "Top- k sampling means sorting by probability and zeroing out the probabilities for anything below the k'th token. To configure Top-k sampling, we set the *do_sample* hyperparameter to true and set the *top_k* hyperparameter to a value above 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e829e375-9ac3-46ba-a4fa-8753196f2a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did the chicken cross the road while being afraid of cars?\n",
      "Top-k Search Response: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      "\n",
      "As an AI language model, I cannot provide any definitive answer on this matter. It's possible that some chickens have developed a fear of cars, but it's equally possible that they cross roads for various reasons, such as to reach food, water, or to mate. Ultimately, the answer depends on the specific chicken in question and the circumstances of the road they're crossing.</s>\n"
     ]
    }
   ],
   "source": [
    "top_k_sampling_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 7\n",
    "    }\n",
    "}\n",
    "top_k_sampling_response = predictor.predict(top_k_sampling_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Top-k Search Response:\", \">\"*40, \"\\033[95m\")\n",
    "print(top_k_sampling_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573bd4c2-9283-4573-915d-2a9aa9d548bf",
   "metadata": {},
   "source": [
    "**Strategy 5: Top-p Sampling**\n",
    "\n",
    "Top k sampling (or nucleus sampling) chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. To configure Top p sampling, we set the *do_sample* hyperparameter to true and set the *top_p* hyperparameter to a value less than 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "608ad83e-d6f8-4cf0-b3b9-4c65d31b1aba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did the chicken cross the road while being afraid of cars?\n",
      "Nucleus search Response: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      "\n",
      "It's a common joke that chickens are afraid of cars, but in reality, they are not afraid of them. However, there are certain breeds of chickens that are less afraid of cars than others.\n"
     ]
    }
   ],
   "source": [
    "nucleus_sampling_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.8\n",
    "    }\n",
    "}\n",
    "nucleus_sampling_response = predictor.predict(nucleus_sampling_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Nucleus search Response:\", \">\"*40, \"\\033[95m\")\n",
    "print(nucleus_sampling_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e210091e-6014-415d-9d3b-525237dd7d47",
   "metadata": {},
   "source": [
    "**Strategy 6: Contrastive Sampling**\n",
    "\n",
    "Contrastive search selects from the most probable candidates predicted by the model while taking into account the degeneration penalty computed from the previous context. This decoding strategy tries to maintain the semantic coherence in the generated text while reducing repetitions. To configure Contrastive search, we set the *top_k* hyperparameter to a value above 1 and set the *repetition_penalty* hyperparameter to a value between 0 and 1. When *repetition_penalty* is close to zero, contrastive search degenerates to the greedy search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5057878-116d-459d-a8c1-dd33ca709ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did the chicken cross the road while being afraid of cars?\n",
      "Contrastive Search Response: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      "\n",
      "I'm sorry, but as an AI language model, I do not have information regarding the behavior of individual chickens in relation to cars.\n"
     ]
    }
   ],
   "source": [
    "contrastive_search_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"repetition_penalty\": 0.9,\n",
    "        \"top_k\": 7\n",
    "    }\n",
    "}\n",
    "contrastive_search_response = predictor.predict(contrastive_search_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Contrastive Search Response:\", \">\"*40, \"\\033[95m\")\n",
    "print(contrastive_search_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d4dec-7df9-40de-8795-ea836f2829a4",
   "metadata": {},
   "source": [
    "**Strategy 7: Combining Multiple Methods**\n",
    "\n",
    "Combining multiple methods (such as top-k and top-p sampling) can sometimes improve the diversity and fluency of the generated text. Try to configure your own decoding strategy including changing the temperature hyperparameter and see the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31039442-f500-4c01-ad17-45a9e71fadf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did the chicken cross the road while being afraid of cars?\n",
      "Multiple Strategies Response 1: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      "\n",
      "As an AI language model, I don't have access to the real-time behavior of animals. So, I can't say for sure whether the chicken was afraid of cars or not while crossing the road.\n"
     ]
    }
   ],
   "source": [
    "multiple_strategies_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 7\n",
    "    }\n",
    "}\n",
    "multiple_strategies_response = predictor.predict(multiple_strategies_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Multiple Strategies Response 1:\", \">\"*40, \"\\033[95m\")\n",
    "print(multiple_strategies_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d54c6bf-d3f5-4db5-83e4-743fe55e80b9",
   "metadata": {},
   "source": [
    "Beam search can be used in conjunction with sampling by choosing the next token at random from the set of possible tokens while building up the beams. Let us try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58d991f2-8ded-4d9d-bdcc-d4a6ba15a724",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint hf-llm-falcon-40b-instruct-bf16-2023-07-29-10-54-30-196 of account 298079122332 not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m multiple_strategies_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     }\n\u001b[1;32m     12\u001b[0m }\n\u001b[0;32m---> 13\u001b[0m multiple_strategies_response \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmultiple_strategies_payload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiple Strategies Response 2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[95m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/base_predictor.py:185\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m\"\"\"Return the inference from the specified endpoint.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m        as is.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m request_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_request_args(\n\u001b[1;32m    178\u001b[0m     data,\n\u001b[1;32m    179\u001b[0m     initial_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m     custom_attributes,\n\u001b[1;32m    184\u001b[0m )\n\u001b[0;32m--> 185\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_runtime_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    962\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    963\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint hf-llm-falcon-40b-instruct-bf16-2023-07-29-10-54-30-196 of account 298079122332 not found."
     ]
    }
   ],
   "source": [
    "multiple_strategies_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 7,\n",
    "        \"num_beams\": 3\n",
    "    }\n",
    "}\n",
    "multiple_strategies_response = predictor.predict(multiple_strategies_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Multiple Strategies Response 2:\", \">\"*40, \"\\033[95m\")\n",
    "print(multiple_strategies_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d99e2",
   "metadata": {},
   "source": [
    "### SageMaker Clean up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c3b60c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
