{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb44f6d",
   "metadata": {},
   "source": [
    "## Text Generation using Different Decoding Strategies with Amazon SageMaker JumpStart SDK and Falcon 40B Instruct Language Model\n",
    "\n",
    "---\n",
    "This Amazon SageMaker Studio Notebook demonstrates how to use the SageMaker Python SDK with very little efforts to firstly deploy Falcon-40B-Instruct Large Language Model and then generate text using different decoding methods. \n",
    "\n",
    "This notebook has the following prerequisites:\n",
    "- Select an AWS region where [Amazon SageMaker JumpStart](https://aws.amazon.com/sagemaker/jumpstart) is available. \n",
    "- [Setup Amazon SageMaker Domain](https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html).\n",
    "- [Available service queta for \"ml.g5.12xlarge for endpoint usage\"](https://docs.aws.amazon.com/general/latest/gr/sagemaker.html).\n",
    "- Less than $10 per hour to spend on Amazon SageMaker JumpStart model deployment and Amazon SageMaker Studio notebook usage.  \n",
    "\n",
    "This notebook is based on the following references:\n",
    "- [Amazon SageMaker JumpStart SDK](https://sagemaker.readthedocs.io/en/v2.82.0/overview.html#use-prebuilt-models-with-sagemaker-jumpstart), providing pretrained models for a wide range of problem types to help you get started with machine learning.\n",
    "- [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct), a top performing open source model with 40B parameters causal decoder-only model built by TII.\n",
    "- Public articles ([Link 1](https://huggingface.co/blog/how-to-generate), [Link 2](https://huggingface.co/docs/transformers/generation_strategies), [Link 3](https://huggingface.co/blog/sagemaker-huggingface-llm#4-run-inference-and-chat-with-our-model)) published on Hugging Face, an open source community and data science platform for machine learning models and datasets. \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b05b931-992e-4526-978d-f03196874a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip --quiet --disable-pip-version-check --root-user-action=ignore\n",
    "!pip install --upgrade sagemaker --quiet --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08508ffa-90f4-4c8c-9c69-b8002bb37f12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model id = huggingface-llm-falcon-40b-instruct-bf16\n",
      "Model name = hf-llm-falcon-40b-instruct-bf16-2023-07-03-00-44-29-841\n",
      "Model version = *\n",
      "Instance type = ml.g5.12xlarge\n",
      "Instance number of GPUs = 4\n",
      "Model maximum input length = 1024\n",
      "Model maximum total tokens = 2048\n",
      "Server endpoint timeout = 300 seconds\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "# Define SageMaker JumpStart Model using model id, instance type, and endpoint timeout\n",
    "my_model = JumpStartModel(model_id=\"huggingface-llm-falcon-40b-instruct-bf16\",\n",
    "                          instance_type=\"ml.g5.12xlarge\",\n",
    "                          env={'ENDPOINT_SERVER_TIMEOUT':'300'})\n",
    "\n",
    "# Take a look at the JumpStart Model parameters printed by this cell\n",
    "print(\"Model id =\", my_model.model_id)\n",
    "print(\"Model name =\", my_model.name)\n",
    "print(\"Model version =\", my_model.model_version)\n",
    "print(\"Instance type =\", my_model.instance_type)\n",
    "print(\"Instance number of GPUs =\", my_model.env[\"SM_NUM_GPUS\"])\n",
    "print(\"Model maximum input length =\", my_model.env[\"MAX_INPUT_LENGTH\"])\n",
    "print(\"Model maximum total tokens =\", my_model.env[\"MAX_TOTAL_TOKENS\"])\n",
    "print(\"Server endpoint timeout =\", my_model.env[\"ENDPOINT_SERVER_TIMEOUT\"], \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a2a8e5-789f-4041-9927-221257126653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------!CPU times: user 178 ms, sys: 18.1 ms, total: 196 ms\n",
      "Wall time: 13min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Host the model on the instance and deploy an inference endpoint\n",
    "# Because the model size is >80GB, expecy deploy() to take 15 min!\n",
    "predictor = my_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b13e2-9d0b-4158-8841-a6c07a7d3b45",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "**Decoding Strategies**\n",
    "\n",
    "Large Language Models are designed to \"guess\" the next token having read all the previous ones based on a predefined sampling method. There are several methods (decoding strategies) we can configure for picking this output token, such as Greedy Search, Beam Search, and Contrastive Search.\n",
    "\n",
    "We start by defining the promote which will enable us to test different decoding strategies for the same input text. We will also fix the values of some hyperparameters across all decoding strategies to make easy for us to compare the generated text:\n",
    "- *Temperature* is used to control the randomness of predictions by scaling the logits before applying softmax. The softmax layer in the transformer architecture turns the logits into probabilities (between 0 and 1). A low temperature (below 1) sharpens the probabilities of the predicted words resulting in more conservative and predictable text. A high temperature (above 1) makes the model generates more creative and diverse text resulting in unusual or unexpected words.\n",
    "- *stop* provides a list of input tokens to the model to stop the generation. The generation will stop when one of the tokens is generated.\n",
    "- *max_new_tokens* defines the maximum number of tokens to be generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77efa7b9-4e50-400b-b7c7-8a8ea07d82a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Why didn't the chicken cross the road? Was it afraid of cars?\"\n",
    "stop_keywords = [\"<|endoftext|>\", \"</s>\"]\n",
    "max_new_tokens = 120\n",
    "temperature = 1.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22952a-4992-41ab-b4d2-2cfb15559652",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Strategy 1: Greedy Search**\n",
    "\n",
    "Greedy search is a deterministic method that simply selects the word with the highest probability as its next word. To configure greedy decoding, we set the *do_sample* hyperparameter to false and make sure the *num_beams* hyperparameter is set to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a55aa5-f2ad-4db8-9718-b76f969cffbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why didn't the chicken cross the road? Was it afraid of cars?\n",
      "Greedy Search Response: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      "\n",
      "We may never know why the chicken crossed the road. Some suggest it was to get to the other side, while others say it is a popular joke with no real answer. Regardless, we can be sure the chicken was not afraid of cars, as they did not exist at the time when the expression originated around the 21st century.</s>\n"
     ]
    }
   ],
   "source": [
    "greedy_search_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": False,\n",
    "        \"num_beams\": 1,\n",
    "    }\n",
    "}\n",
    "greedy_search_response = predictor.predict(greedy_search_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Greedy Search Response:\", \">\"*40, \"\\033[95m\")\n",
    "print(greedy_search_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e153983-9964-4e8f-b148-5a2e968d61cc",
   "metadata": {},
   "source": [
    "**Strategy 2: Beam Search**\n",
    "\n",
    "Beam search is another deterministic method that reduces the risk of missing hidden high probability word sequences by keeping a fixed number (beam) of active candidates at each time step and eventually choosing the hypothesis that has the overall highest probability. To configure beam decoding, we set the *do_sample* hyperparameter to false and set the *num_beams* hyperparameter to a value above 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "008e8836-7a49-407f-bf04-8dcf59de8dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why didn't the chicken cross the road? Was it afraid of cars?\n",
      "Beam Search Response: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      " did a fox or other predator attack it? or did the chicken have an unknown phobia towards roads?\n",
      "As an AI language model, I cannot determine the reason why the chicken did not cross the road, as it could be a multitude of reasons based on its behavior or the environment.\n"
     ]
    }
   ],
   "source": [
    "beam_search_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": False,\n",
    "        \"num_beams\": 4,\n",
    "    }\n",
    "}\n",
    "beam_search_response = predictor.predict(beam_search_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Beam Search Response:\", \">\"*40, \"\\033[95m\")\n",
    "print(beam_search_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf74caa-c563-4b1e-9585-a28237dc539f",
   "metadata": {},
   "source": [
    "**Strategy 3: Multinomial Sampling**\n",
    "\n",
    "Solely maximizing the output probability in deterministic methods can lead to dullness and repetitions. Conversely, stochastic methods try to solve the problem by introducing randomness to the sampling process. \n",
    "\n",
    "Multinomial sampling randomly selects the next token based on the probability distribution over the entire vocabulary given by the model. Every token with a non-zero probability has a chance of being selected, thus reducing the risk of repetition. To configure multinomial sampling, we set the *do_sample* hyperparameter to true and set the *num_beams* hyperparameter to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b253a50c-a024-4608-9502-00591764eb5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why didn't the chicken cross the road? Was it afraid of cars?\n",
      "Multinomial Sampling Response: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      "\n",
      "The reason behind the chicken crossing the road might vary depending on the situation. It could be due to the attraction towards the other side of the road, distraction, or fear of cars.\n"
     ]
    }
   ],
   "source": [
    "multinomial_sampling_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "         \"do_sample\": True,\n",
    "        \"num_beams\": 1\n",
    "    }\n",
    "}\n",
    "multinomial_sampling_response = predictor.predict(multinomial_sampling_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Multinomial Sampling Response:\", \">\"*40, \"\\033[95m\")\n",
    "print(multinomial_sampling_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ad903-7381-466a-b0e1-33cd61532786",
   "metadata": {},
   "source": [
    "**Strategy 4: Top-k Sampling**\n",
    "\n",
    "Top- k sampling means sorting by probability and zeroing out the probabilities for anything below the k'th token. To configure Top-k sampling, we set the *do_sample* hyperparameter to true and set the *top_k* hyperparameter to a value above 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e829e375-9ac3-46ba-a4fa-8753196f2a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why didn't the chicken cross the road? Was it afraid of cars?\n",
      "Top-k Search Response: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      "\n",
      "There is actually no definitive answer to why the chicken didn't cross the road. The joke has been told many ways, with different punchlines. Some people say the chicken was too afraid to cross the road, while others say it didn't want to get hit by a car. Still others say it was simply a chicken and didn't feel like crossing the road.\n"
     ]
    }
   ],
   "source": [
    "top_k_sampling_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 6\n",
    "    }\n",
    "}\n",
    "top_k_sampling_response = predictor.predict(top_k_sampling_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Top-k Search Response:\", \">\"*40, \"\\033[95m\")\n",
    "print(top_k_sampling_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573bd4c2-9283-4573-915d-2a9aa9d548bf",
   "metadata": {},
   "source": [
    "**Strategy 5: Top-p Sampling**\n",
    "\n",
    "Top k sampling (or nucleus sampling) chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. To configure Top p sampling, we set the *do_sample* hyperparameter to true and set the *top_p* hyperparameter to a value less than 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "608ad83e-d6f8-4cf0-b3b9-4c65d31b1aba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why didn't the chicken cross the road? Was it afraid of cars?\n",
      "Nucleus search Response: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      "\n",
      "I'm sorry, I cannot answer that question as it is a well-known joke that cannot be accurately interpreted.\n"
     ]
    }
   ],
   "source": [
    "nucleus_sampling_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.8\n",
    "    }\n",
    "}\n",
    "nucleus_sampling_response = predictor.predict(nucleus_sampling_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Nucleus search Response:\", \">\"*40, \"\\033[95m\")\n",
    "print(nucleus_sampling_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e210091e-6014-415d-9d3b-525237dd7d47",
   "metadata": {},
   "source": [
    "**Strategy 6: Contrastive Sampling**\n",
    "\n",
    "Contrastive search selects from the most probable candidates predicted by the model while taking into account the degeneration penalty computed from the previous context. This decoding strategy tries to maintain the semantic coherence in the generated text while reducing repetitions. To configure Contrastive search, we set the *top_k* hyperparameter to a value above 1 and set the *repetition_penalty* hyperparameter to a value between 0 and 1. When *repetition_penalty* is close to zero, contrastive search degenerates to the greedy search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5057878-116d-459d-a8c1-dd33ca709ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why didn't the chicken cross the road? Was it afraid of cars?\n",
      "Contrastive Search Response: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      " Was it afraid of the road? Was it afraid of the chicken's chicken? Was it afraid of the chicken's chicken's chicken? Was it afraid of the chicken's chicken's chicken's chicken? Was it afraid of the chicken's chicken's chicken's chicken's chicken? Was it afraid of the chicken's chicken's chicken's chicken's chicken's chicken? Was it afraid of the chicken's chicken's chicken's chicken's chicken's chicken's chicken? Was it afraid of the chicken's\n"
     ]
    }
   ],
   "source": [
    "contrastive_search_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": True,\n",
    "        \"repetition_penalty\": 0.6,\n",
    "        \"top_k\": 6\n",
    "    }\n",
    "}\n",
    "contrastive_search_response = predictor.predict(contrastive_search_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Contrastive Search Response:\", \">\"*40, \"\\033[95m\")\n",
    "print(contrastive_search_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d4dec-7df9-40de-8795-ea836f2829a4",
   "metadata": {},
   "source": [
    "**Strategy 7: Combining Multiple Methods**\n",
    "\n",
    "Combining multiple methods (such as top-k and top-p sampling) can sometimes improve the diversity and fluency of the generated text. Try to configure your own decoding strategy including changing the temperature hyperparameter and see the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31039442-f500-4c01-ad17-45a9e71fadf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why didn't the chicken cross the road? Was it afraid of cars?\n",
      "Multiple Strategies Response: >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> \u001b[95m\n",
      "\n",
      "The chicken crossed the road to get to the other side. There is no indication that it was afraid of cars.\n"
     ]
    }
   ],
   "source": [
    "multiple_strategies_payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"stop\": stop_keywords,\n",
    "        \"temperature\": 1,\n",
    "        \"max_new_tokens\": 120,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 6\n",
    "    }\n",
    "}\n",
    "multiple_strategies_response = predictor.predict(multiple_strategies_payload)\n",
    "\n",
    "print(prompt)\n",
    "print(\"Multiple Strategies Response:\", \">\"*40, \"\\033[95m\")\n",
    "print(multiple_strategies_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d99e2",
   "metadata": {},
   "source": [
    "### SageMaker Clean up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c3b60c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
